[project]
name = "medgemma-assistant"
version = "0.1.0"
description = "AI-powered clinical decision support with MedGemma"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "accelerate>=1.12.0",
    "aiofiles>=25.1.0",
    "bitsandbytes>=0.49.1",
    "fastapi>=0.128.4",
    "fhir-resources>=8.2.0",
    "httpx>=0.28.1",
    "huggingface-hub>=0.30.0",
    "jinja2>=3.1.6",
    "numpy>=2.4.2",
    "pillow>=12.1.0",
    "pydantic>=2.12.5",
    "python-dotenv>=1.0.0",
    "python-multipart>=0.0.22",
    "scipy>=1.17.0",
    "sounddevice>=0.5.5",
    "torch>=2.10.0",
    "transformers>=5.1.0",
    "uvicorn[standard]>=0.40.0",
    "websockets>=16.0",
]

[project.optional-dependencies]
# vLLM requires specific torch version - install separately if needed
# pip install vllm (will install compatible torch version)
vllm = []


